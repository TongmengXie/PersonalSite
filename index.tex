% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[normalem]{ulem}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Learning Diary - CASA0023},
  pdfauthor={Tongmeng Xie},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Learning Diary - CASA0023}
\author{Tongmeng Xie}
\date{2023/3/16}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, borderline west={3pt}{0pt}{shadecolor}, breakable, enhanced, interior hidden, sharp corners, frame hidden]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\bookmarksetup{startatroot}

\hypertarget{introduction}{%
\chapter*{Introduction}\label{introduction}}
\addcontentsline{toc}{chapter}{Introduction}

Welcome to my learning diary page of Remote Sensing Cities and
Environment (CASA0023)! This diary is made for the content taught at
2022-2023.

I'm a current Master of Science student at Bartlett Centre for Advanced
Spatial Analysis

\bookmarksetup{startatroot}

\hypertarget{week-1}{%
\chapter{Week 1}\label{week-1}}

\hypertarget{summary}{%
\section{Summary}\label{summary}}

This section summarises the lecture content and a graph of feature space
derived from practical in SNAP operations.

Passive data: Energy usually in eletcromagnetic form e.g., human eyes

Active data: Energy in addition in illumination . e.g., radar.

How EM waves interact with Earth's surface and atmosphere: Reflection,
scattering, absorption

single

dual

quad

\hypertarget{remotely-sensed-data-usually-comes-in}{%
\subsection{remotely-sensed data usually comes
in}\label{remotely-sensed-data-usually-comes-in}}

Raster: file

types: BIL, BSQM BIP, GeoTIFF

\hypertarget{four-resolutions}{%
\subsection{Four resolutions:}\label{four-resolutions}}

Spatial: ranging from 10 cm to several kilos

Spectral: How many different spectral bands are there? (Every feature on
earth has a unique spectral signature)

(Atmospheric windows: )

(Vegetation: red edge -\/- infra bands. APP: look at the infra band s of
city to identify who has access to vegetation)

Radiometric resolution: resolution of cell's value

Temporal resolution: ussualy inversly relate3 to pixel size (spatial)

MODIS

\hypertarget{practical}{%
\subsection{Practical}\label{practical}}

\begin{figure}

{\centering \includegraphics{./images/spectral_feature_space_vege_B04_B08.png}

}

\caption{\label{fig-vege}Spectral Feature Space, Vegetation On Bands B04
and B08}

\end{figure}

\hypertarget{application}{%
\section{Application:}\label{application}}

``Spectral Feature Space, Vegetation On Bands B04 and B08''

One of the applications really attracted me was the spatial signature of
vegetation on the terra, as we could assign features to each end of the
spatial signature area see Figure~\ref{fig-vege}, such as bare land on
the right end of the triangle-like area where red light captured are
dense while near-infrared level is low. Heavy vegetation are witnessed
at the upper end of the triangle-like area where red light low and
near-infrared is high, indicating heavy biomass. As for the left-down
corner where both red and near-infrared are low, we can identify wet
lands. This is integrated in the NDVI (Normalized Difference Vegetation
Index) to estimate vegetation cover.

Spatial signatures can also be used to monitor the health of vegetation
by identifying patterns of quavariation in spectral reflectance that are
indicative of stress or disease. For example, vegetation that is
stressed or diseased may have a different spectral reflectance signature
than healthy vegetation, which can be identified using spatial
signatures.

In addition, spatial signatures can be used to monitor the growth and
distribution of vegetation over time by comparing satellite imagery from
different dates. This can be useful for understanding the impacts of
land use changes, climate change, and other factors on vegetation.

Overall, spatial signatures are a powerful tool for vegetation
monitoring, as they can be used to identify and classify different types
of vegetation, monitor vegetation health, and track vegetation changes
over time.

\hypertarget{reflection}{%
\section{Reflection}\label{reflection}}

just state what interest you and why, as well as the application.
Application: Context matters. Why useful? What had it assisted
achieving. Mind map of concepts, to show understanding of data and
workflow

One of the challenges I encountered is to navigate the complexities of
the interface of SNAP and QGIS. It becomes clear to me that yes
implementing several functions in code can be challenging, but a
software with collective functions as a whole can be mindblowing even
when with decent GUIs. Specifically, finding which function falling
under which menu consumes a lot of time, and figuring out filling
parameters to carry the analysis also took some efforts of iterative
validation.

When doing the operation in R on a script level, it becomes confusing
where I put the data

\bookmarksetup{startatroot}

\hypertarget{week-2---portfolio}{%
\chapter{2. Week 2 - Portfolio}\label{week-2---portfolio}}

\bookmarksetup{startatroot}

\hypertarget{week-3---remote-sensing-data}{%
\chapter{Week 3 - Remote sensing
data}\label{week-3---remote-sensing-data}}

In this week's learning diary, we try to handle

\hypertarget{summary-1}{%
\section{Summary:}\label{summary-1}}

\hypertarget{different-sensors}{%
\subsection{Different Sensors}\label{different-sensors}}

Across track scanners: Mirror reflects light onto 1 detector. For
example, \uline{Landsat} dataset are captured by this sort

Along track scanners: Basically several detectors pushed along. E.g.,
Quickbird, SPOT

\hypertarget{geometric-correction}{%
\subsection{Geometric Correction}\label{geometric-correction}}

RS data could include image distortions introduced by: View angle,
topography, wind and rotation of the earth

We identify Ground Control Points (GCP) in distorted data to match them
with local map, correct image, or GPS data from handheld device, but
these reference images could also contain distortions and imprecisions.

RMSE is adopted here to measure fitness between images. Use GCPs to
minimise RMSE.

Doing geometric correction can shift the original image, so we want to
re-sample the final raster by using Nearest Neighbour, Linear, Cubic,
Cubic spline re-samplers

\hypertarget{atmosphric-correction}{%
\subsection{Atmosphric Correction}\label{atmosphric-correction}}

According to Jensen (1986), two factors contribute to environmental
attenuation: Atmospheric scattering, topographic attenuation.

There are unnecessary and necessary atmospheric corrections:

necessary ones are:

\begin{itemize}
\item
  Biophysical parameters needed (e.g.~temperature, leaf area index,
  NDVI)
\item
  E.g. .. .NDVI is used in the Africa Famine Early Warning System and
  Livestock Early Warning System
\item
  Using spectral signatures through time and space
\end{itemize}

Absorption and scattering can create the haze, i.e.~reduces contrast of
image.

Scattering can create the ``adjacency effect'', radiance from pixels
nearby mixed into pixel of interest.

\hypertarget{orthorectification-correction}{%
\subsection{Orthorectification
Correction}\label{orthorectification-correction}}

This is a subset of georectification, i.e.~giving coords to an image.
Particularly Orthorectification means removing distortion so pixels can
appear being viewed at \uline{nadir} (straight down). This requires the
support of an Elevation Model to calculate the nadir view for each pixel
on a sensor geometry.

To do this: cosine correction, Minnaert correction, Statistical
Empirical correction, C Correction (advancing the Cosine). Need radiance
(DN to TOA) from sloped terrain, Sun's zenith angle, Sun's incidence
angle - cosine of the angle between the solar zenith and the normal line
of the slope. Latter two found in angle coefficient files (e.g.~Landsat
data ANG.txt).

\hypertarget{rdiometric-correction}{%
\subsection{Rdiometric Correction}\label{rdiometric-correction}}

Corrections to raw satellite imagery can be performed using a method
called Dark Object Subtraction (DOS). The logic is that the darkest
pixel in the image should be 0 and any value it has is due to the
atmosphere. To remove the atmospheric effect, the value from the darkest
pixel is subtracted from the rest of the pixels in the image. The
calculation involves converting the Digital Number (DN) to radiance,
computing the haze value for each band (but not beyond NIR), and
subtracting the 1\% reflectance value from the radiance. The calculation
requires values such as mean exoatmospheric irradiance, solar azimuth,
Earth-sun distance, and others, which can be found in sources such as
Landsat user manuals.

\hypertarget{joining-data-sets}{%
\subsection{Joining data sets}\label{joining-data-sets}}

Also known as \uline{Mosaicking}: We feather two images, creating a
\uline{seamless} mosaic, where the diving lien is called
\uline{seamline}.

\hypertarget{image-enhancements}{%
\subsection{Image Enhancements}\label{image-enhancements}}

Image stretch, Band ratioing, Normalised Burn Ratio, Edge enhancement,
Filtering, PCA, Image fusion (see application) etc.

\hypertarget{application---discussing-image-fusion-in-one-literature}{%
\section{Application - Discussing image fusion in one
literature}\label{application---discussing-image-fusion-in-one-literature}}

\includegraphics{./images/image-620878088.png}

From literature we delve in the nuances of levels on which we perform
image fusion to acquire better results. The integration methods vary as
the levels vary (Schulte to B端hne and Pettorelli 2018).

Satellite remote sensing (SRS) can be derived from Multispectral sensors
and radar sensors.~

~Multispectral sensors are passive, merely receiving electromagnetic
waves reflected from surface, usually used to reflect chemical
properties (such as nitrogen or carbon content and moisture). Usually
produces data with comparatively low spatial resolution

~Radar ones emit electromagnetic radiation and measure the returning
signal, responding to the three-dimensional structure of objects, being
sensitive to their orientation, volume and surface roughness. Usually
produces data with comparatively high spatial resolution

\hypertarget{image-fusion}{%
\subsection{Image fusion:}\label{image-fusion}}

1.~\textbf{decision-level}~(SRS integration), where separate predictors
are used to estimate a parameter of interest.~

2.~\textbf{object-level (feature-level).}~unit: multi-pixel objects. (1)
using radar and multispectral imagery is input into an object-based
image segmentation algorithm, or (2) segmenting each type of imagery
separately before combining them. multi-pixel objects

3.~\textbf{pixel-level (Observation-level)}, where pixel values are
combined to derive a fused image with new pixel values, either in the
spatial or the temporal domain.

(2. and 3. derive entirely new predictors.)

\begin{figure}

{\centering \includegraphics{./images/fusion techniques.png}

}

\caption{\label{fig-fusiontech}Credit: Schulte to B端hne and Pettorelli
(2018)}

\end{figure}

Schematic overview of multispectral-radar SRS data fusion techniques.
The parameter of interest can be a categorical variable, like land
cover, or a continuous variable, like species richness. In pixel-level
fusion, the original pixel values of radar and multispectral imagery are
combined to yield new, derived pixel values. Object-based fusion refers
to (1) using radar and multispectral imagery is input into an
object-based image segmentation algorithm, or (2) segmenting each type
of imagery separately before combining them. Finally, decision-level
fusion corresponds to the process of quantitatively combining
multispectral and radar imagery to derive the parameter of interest (by
e.g.~combining them in a regression model, or classification algorithm)

\hypertarget{implementation-approaches}{%
\subsection{Implementation Approaches}\label{implementation-approaches}}

\begin{figure}

{\centering \includegraphics{./images/Implementation approach.png}

}

\caption{\label{fig-impleApproach}Credit: Schulte to B端hne and
Pettorelli (2018)}

\end{figure}

\textbf{\emph{pixel-level}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Component substitution techniques: such as principal component
  analysis (PCA), Intensity-hue-saturation (IHS).~\\
\item
  PCA is the only pixel-level image fusion technique that cannot be
  applied to imagery with different spatial resolutions, and the only
  that allows unlimited image numbers.\\
\item
  IHS fusion. Three images with lower spatial resolution (typically
  multispectral data) are integrated with a single image with high
  spatial resolution (typically radar) to retain the radiometry but
  increase the spatial resolution of the former. Facilitate visual
  interpretation by combining resulting images into a single RGB
  image.\\
\item
  Multi-resolution analysis,~such as~**Wavelet transformation. Decompose
  multispectral and radar imagery into their respective low- and
  high-frequency components\\
\item
  Arithmetic fusion techniques:~such as the Brovey transform algorithm.
  Unlikely to be appropriate for multispectral-radar SRS image fusion.
\end{enumerate}

\textbf{\emph{Object-level}}:~Based on brightness and intensity values
of each pixel, as well as its spatial context, objects such as lines,
shapes or textures are extracted.

1.~\textbf{image segmentation:} Demands that multispectral and radar SRS
images are with the same spatial resolution

2.~*extracting objects separately and combining in a feature map*

Object-based fusion reduces all multispectral and radar information into
a single layer of discrete objects, which are often relatively easy to
relate to ecological features.

\textbf{\emph{Decision-level fusion}}: Quantitative decision-making
frameworks---such as a regression, a quantitative model or a
classification algorithm.

\hypertarget{reflection-1}{%
\section{Reflection}\label{reflection-1}}

Data correction, Data fusion and Image enhancement SRS data fusion can
increase the quality of SRS (Satellite Remote sensing)-derived
parameters for application in terrain detection, urban analysis, ecology
and conservation (Schulte to B端hne and Pettorelli 2018). It is thus
important to explore how best to capitalise on recent technological
developments and changes in SRS data availability. It is exctiing to
apply solid machine learning methods to this area and it is marvelous to
see the progress reflected by the increasing number of software
supporting this application. The improvement of image quality enables
new research designs in ecology and conservation areas and reignite
previously greyed-out options.

The application of data correction, data fusion, and image enhancement
techniques to SRS data can greatly improve the accuracy and reliability
of SRS-derived parameters, which can then be used in various fields,
including terrain detection, urban analysis, ecology, and conservation.
With the rapid advancements in technology and the increasing
availability of SRS data, there is a growing opportunity to leverage the
latest machine learning techniques in this area. The development of new
software tools to support these applications is a testament to the
progress being made in this field. By enhancing the quality of the SRS
data, researchers are able to design more robust and informative
studies, unlocking new insights and avenues for exploration in ecology
and conservation. This, in turn, has the potential to lead to
breakthroughs and innovations in these fields, making a significant
impact on the world around us.

\bookmarksetup{startatroot}

\hypertarget{week4---policy-applications}{%
\chapter{Week4 - Policy
applications}\label{week4---policy-applications}}

\hypertarget{summary-2}{%
\section{Summary}\label{summary-2}}

\hypertarget{sensor-data}{%
\subsection{Sensor Data}\label{sensor-data}}

\bookmarksetup{startatroot}

\hypertarget{week-5---an-introduction-to-google-earth-engine}{%
\chapter{Week 5 - An introduction to Google Earth
Engine}\label{week-5---an-introduction-to-google-earth-engine}}

This week introduces \textbf{Google Earth Engine (GEE)}, a geospatial
processing service that allows for planetary scale analysis of massive
datasets in seconds.

Basics:

\begin{itemize}
\tightlist
\item
  The set up of GEE, its terms and jargon, and client vs server side
  operations, see Table 1
\item
  How GEE uses Javascript and how mapping functions are used instead of
  loops
\item
  The concept of scale in GEE, which refers to both the volume of
  analysis and pixel resolution
\item
  How GEE aggregates the image to fit a 256x256 grid.
\end{itemize}

Objects and methods in GEE are introduced:

\begin{itemize}
\tightlist
\item
  E.g. geometries, features, feature collections, and
\item
  Various data reduction techniques (e.g., reducing images, reducing
  images by region(s), reducing images by neighborhood).
\end{itemize}

Also, the types of analyses that can be performed in GEE are briefly
covered.

\hypertarget{summary-3}{%
\section{Summary}\label{summary-3}}

\hypertarget{gee-basics}{%
\subsection{GEE Basics}\label{gee-basics}}

JavaScript, where objects are dictionaries:

\begin{itemize}
\tightlist
\item
  We have ee (EarthEngine), a powerful package. Anything starting with
  ee (proxy objects) are stored on the server.
\item
  Problems:

  \begin{itemize}
  \tightlist
  \item
    We don't iterate the data on the server; instead, we map (using a
    mapping function) them into objects (variables) so we only load them
    once.
  \item
    There are also some sort of server-wide functions.
  \item
    Avoid using loops in GEE on the server-side, as mapping can
    automatically detect the number of loops needed.
  \end{itemize}
\end{itemize}

Scale:

\begin{itemize}
\tightlist
\item
  Pixel resolution, set by the output.
\item
  GEE does resampling, aggregating your input to a 256*256, mainly
  down-sampling.
\end{itemize}

Table 1: Terms and Jargon Related to Google Earth Engine

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Term
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} \\
\midrule()
\endhead
Google Earth Engine & A geospatial processing service that allows
geospatial analysis at scale. \\
Image & Refers to raster data in GEE and has bands. \\
Feature & Refers to vector data in GEE and has geometry and
attributes. \\
ImageCollection & A stack of images in GEE. \\
FeatureCollection & A stack of features (lots of polygons) in GEE. \\
Proxy objects & GEE objects that are stored on the server and have no
data in the script. \\
\bottomrule()
\end{longtable}

Table 2: Differences between Client and Server Side in Google Earth
Engine

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} \\
\midrule()
\endhead
Client Side & Refers to the browser side of GEE. \\
Server Side & Refers to the side of GEE where data is stored. \\
Earth Engine Objects & Objects in GEE starting with ``ee''. \\
Looping & Looping is not recommended for objects on the server side. \\
Mapping & Instead of loops, mapping is used in GEE to apply a function
to everything on the server. \\
Scale & Scale refers to pixel resolution in GEE. The scale is set by the
output, not the input, and Earth Engine selects the pyramid with the
closest scale to analysis. \\
\bottomrule()
\end{longtable}

\hypertarget{gee-objects}{%
\subsection{GEE Objects}\label{gee-objects}}

Objects:

\begin{itemize}
\tightlist
\item
  Images (Rasters), geometry, ImageCol, features, featureCol, joins,
  arrays, chart.
\end{itemize}

Table 3: Geometry Types and Features

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Type of Geometry
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule()
\endhead
Point & A single location represented by its longitude and latitude \\
Line & A series of connected points representing a linear feature \\
Polygon & A closed shape with three or more sides, represented by a
series of connected lines forming a closed loop \\
MultiPolygon & A collection of polygons, where each polygon is
represented as a list of coordinate tuples defining its vertices \\
MultiGeometry & A collection of different types of geometries \\
\bottomrule()
\end{longtable}

\hypertarget{gee-processes-and-applicationsoutputs}{%
\subsection{GEE Processes and
Applications/Outputs}\label{gee-processes-and-applicationsoutputs}}

GEE applications:

\begin{itemize}
\tightlist
\item
  Reducing types.
\item
  Different to filterBounds() that filters the area of interest, to do
  zonal statistics, we have reduceRegion(), where regions are
  subcategories of the area of interest.
\item
  Also, we have reduceNeighborhood(), which is a bit like a kind of
  image enhancement.
\end{itemize}

Linear Regressions:

\begin{itemize}
\tightlist
\item
  In a scenario of visualising precipitation, we can do a multivariate
  multiple linear regression where both independent variables (time) and
  dependent (precip, temp) variables are multiple.
\item
  Something about constant bound.
\end{itemize}

Joins:

\begin{itemize}
\tightlist
\item
  In GEE, everything, e.g.~within a buffer, intersect, etc. needs the
  mediation of Join (apply()).
\item
  To perform joins, we need to put data into Filter().
\end{itemize}

Classifiers:

\begin{itemize}
\tightlist
\item
  Per-pixel
\item
  sub-pixel
\end{itemize}

Table 4: GEE Processes and Applications/Outputs

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Process
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule()
\endhead
Geometry operations & Spatial operations such as union, intersection,
buffer, and distance analysis \\
Joins & Combining two feature collections based on a shared attribute
value \\
Zonal statistics & Computing statistics for a region or set of regions
such as mean, median, and mode of pixel values within a feature or a
collection of features \\
Filtering & Filtering of images or specific values based on criteria
such as date range, location, and attribute value \\
Machine learning & Using statistical and machine learning algorithms for
classification, clustering, and prediction tasks \\
Deep learning & A subset of machine, using Deep Neural Networks \\
\bottomrule()
\end{longtable}

\hypertarget{limitations}{%
\subsection{Limitations}\label{limitations}}

No support for phase data, needs SNAP.

\hypertarget{application-1}{%
\section{Application}\label{application-1}}

\hypertarget{reflection-2}{%
\section{Reflection}\label{reflection-2}}

GEE-using skills can be a valuable asset for a spatial data scientist,
as it allows for complex spatial analysis at scale. Traditional GIS
software is eclipsed when it comes to both efficiency and scale.

GEE's unique and efficient way of conducting analysis flows is
interesting, such as the introduction of concepts like client vs
server-side operations and data reduction techniques. These was required
by GEE's feature of carrying out analyses on massive datasets (Gorelick
et al. 2017). For those interested in BigData technology, the strategies
(server/client split, no looping on server, etc.) applied by Google here
is a very resourceful one and worth learning. The user end also has to
learn to adopt good practices for reducing data range, which has been
simplified to a series of reduction and filtering functions,
e.g.~\texttt{ImageCollection.filterDate(),\ image.reduceNeighborhood()}(Google
2023b).

GEE's combination with machine learning is also promising in regard of
automating complex analysis tasks, as Machine Learning APIs offered by
GEE support Supervised and Unsupervised Classification, and Regression
(Google 2023a). According to Saad El Imanni et al. (2023), as a subtask
of intelligent agriculture, weeds detection task sees an impressive
performance (overall accuracy reached 96.87\%) when GEE and Machine
learning are combined.

\bookmarksetup{startatroot}

\hypertarget{wk6-classification}{%
\chapter{Wk6 Classification}\label{wk6-classification}}

\hypertarget{summary-4}{%
\section{Summary}\label{summary-4}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Information
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Summary
\end{minipage} \\
\midrule()
\endhead
Purpose of classification & To subset data into classes or values, such
as landcover or estimating values like GCSE scores or pollution. \\
Different classification methods & Essentially slice the data in
different ways. \\
Complexity of classification methods & They can often be made to appear
more complicated than they are. \\
Controlling classifiers & Can be done using hyperparameters. \\
Desired outcome of classifiers & Can range from a single tree to a
decision hyperplane boundary in multiple dimensions. \\
\bottomrule()
\end{longtable}

\hypertarget{ml-methods-in-eo-data-classification}{%
\subsection{ML methods in EO data
classification}\label{ml-methods-in-eo-data-classification}}

Table 1: Supervised Classification Methods

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule()
\endhead
Maximum Likelihood & A statistical method used to estimate the
parameters of a probability distribution based on observed data. \\
Support Vector Machines (SVM) & A supervised learning algorithm that
finds the best hyperplane to separate data into different classes. \\
\bottomrule()
\end{longtable}

Table 2: Unsupervised Classification Methods

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule()
\endhead
Density Slicing & Divides the range of pixel values into equal intervals
and assigns each interval a unique class value. \\
Parallelpiped & Uses a set of user-defined ranges for each band to
define class boundaries in multi-dimensional space. \\
Minimum Distance to Mean & Assigns each pixel to the class with the
closest mean value in multi-dimensional space. \\
Nearest Neighbor & Assigns each pixel to the class of its nearest
neighbor in multi-dimensional space. \\
\bottomrule()
\end{longtable}

Table 3: Other Machine Learning Methods

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule()
\endhead
Artificial Neural Networks (ANN) & A set of algorithms inspired by the
structure and function of biological neural networks, used for pattern
recognition and prediction tasks. \\
\bottomrule()
\end{longtable}

\hypertarget{pros-and-cons---supervised-vs.-unsupervised}{%
\subsection{Pros and cons - Supervised
vs.~Unsupervised}\label{pros-and-cons---supervised-vs.-unsupervised}}

Table 1: Supervised vs.~Unsupervised Classification

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Classification Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} \\
\midrule()
\endhead
Supervised & Classifier learns patterns in the data and uses that to
place labels onto new data. Pattern vector is used to classify the
image. Usually, pixels are treated in isolation but as we have seen -
contextual (neighboring pixels), objects (polygons), texture. & Pattern
recognition or machine learning \\
Unsupervised & Identifies land cover classes that aren't known a priori
(before) and tells the computer to cluster based on info it has
(e.g.~bands) and label the clusters. & Density slicing, parallelpiped,
minimum distance to mean, nearest neighbor, neural networks, machine
learning / expert systems* \\
\bottomrule()
\end{longtable}

\hypertarget{overfitting}{%
\subsection{Overfitting}\label{overfitting}}

\begin{itemize}
\tightlist
\item
  Bias refers to the difference between the predicted value and the true
  value. When a model has high bias, it is too simple and may underfit
  the data. On the other hand, when a model has low bias, it may overfit
  the data.
\item
  Variance, on the other hand, refers to the variability of a model for
  a given point. When a model has high variance, it is too complex and
  may overfit the data. This means that it will perform well on the
  training data but poorly on new data.
\end{itemize}

\begin{figure}

{\centering \includegraphics{https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bf6f589e-d103-4a08-802c-8f2b8eb0cf7e/Untitled.png}

}

\caption{Untitled}

\end{figure}

Credit: CASA0006

In general, overfitting occurs when there is a trade-off between bias
and variance. A model with high bias and low variance will underfit the
data, while a model with low bias and high variance will overfit the
data. The goal is to find a balance between bias and variance that
results in good performance on both training and test data.

\begin{figure}

{\centering \includegraphics{https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2d9577b9-f0d0-4cd5-b5c2-bf40f3081d1e/Untitled.png}

}

\caption{Untitled}

\end{figure}

Credit: CASA0006

\hypertarget{outlook-on-the-development-of-eo-data-classification}{%
\subsection{Outlook on the development of EO data
Classification}\label{outlook-on-the-development-of-eo-data-classification}}

Earth Observation (EO) data classification is continually evolving, with
new technologies and techniques leading to several anticipated future
developments:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Multi-source data fusion:

  \begin{itemize}
  \tightlist
  \item
    Integrating data from multiple sources like satellite imagery,
    LiDAR, and ground-based sensors will become more prevalent. This
    fusion enhances classification accuracy and offers comprehensive
    Earth's surface information, improving decision-making and
    monitoring. For example, the European Union's
    \textbf{\href{https://www.copernicus.eu/en}{Copernicus Programme}}
    could use this in providing free data from various satellite
    missions and sensors for environmental monitoring, disaster
    management, and urban planning.
  \end{itemize}
\item
  Multi-temporal analysis:

  \begin{itemize}
  \tightlist
  \item
    Sophisticated multi-temporal analysis techniques will be
    increasingly used to monitor changes in land cover, vegetation, and
    other features over time. This enables accurate and efficient change
    detection and monitoring of phenomena like urbanization,
    deforestation, and climate change. This aligns with the
    \textbf{\href{https://unfccc.int/topics/land-use/workstreams/reddplus}{REDD+}}
    initiative under the United Nations Framework Convention on Climate
    Change (UNFCCC), which uses multi-temporal analysis to monitor
    forest cover changes and evaluate policy effectiveness in reducing
    greenhouse gas emissions from deforestation and forest degradation.
  \end{itemize}
\item
  Cloud-based processing:

  \begin{itemize}
  \tightlist
  \item
    The growth of cloud-based platforms, such as Google Earth Engine,
    allows for more efficient and scalable EO data processing workflows.
    This accessibility enables researchers and organizations to innovate
    in classification techniques and applications. The National Oceanic
    and Atmospheric Administration's (NOAA)
    \textbf{\href{https://ncics.org/data/noaa-big-data-project/}{Big
    Data Project}} aims to make vast amounts of environmental data
    accessible and usable in the cloud, fostering innovation in
    developing new applications and services.
  \end{itemize}
\end{enumerate}

These advancements will provide comprehensive and timely information
about Earth's surface, informing policies and strategies in areas such
as environmental management, disaster response, and urban planning.

\bookmarksetup{startatroot}

\hypertarget{week7---classification-and-accuracy}{%
\chapter{Week7 - Classification and
Accuracy}\label{week7---classification-and-accuracy}}

This week's learning diary continues that from Week 6 in addressing the
big problem in Remote Sensingm, i.e.~classification within Earth
Observation data. Also, accuracy metrics are discussed.

\hypertarget{summary-5}{%
\section{Summary}\label{summary-5}}

The summary of lecture content as well as practical outcomes. See
\textbf{?@fig-mindmap} for an overview. If certain words are
intelligible due to resolution issues, hopefully you can right click and
``open in new page'' to get a better view since this is a .SVG file.
\includegraphics{./images/MindMap.png}

\hypertarget{data}{%
\subsection{Data}\label{data}}

\begin{itemize}
\item
  Surface Reflectance (SR)
\item
  Top of Atmosphere (TOA)
\end{itemize}

A mixed way of doing urban recognition

\hypertarget{obia-object-based-image-analysis}{%
\subsection{OBIA (object-based image
analysis)}\label{obia-object-based-image-analysis}}

Instead of a per-pixel approach, we adopt an object-based image analysis
(OBIA), where you have to manually create objects.

SLIC (\textbf{\emph{Simple~Linear~Iterative~Clustering}}) (2012): No
ground truth

Descent, similarity (Homogeneity)

\hypertarget{sub-pixel-analysis}{%
\subsection{Sub-pixel analysis}\label{sub-pixel-analysis}}

SMA (Spectral maximum analysis), SPC, LSU

Through a series of manipulation of material, we acquire a list
describing the broken-down land cover of that pixel

\hypertarget{pixel-purity}{%
\subsubsection{Pixel purity}\label{pixel-purity}}

\textbf{Endmember}: an important concept in spectral mixture analysis

In remote sensing, an end member refers to a pure or nearly pure
material or component that is present within a mixed pixel.

In spectral mixture analysis, the spectral signature of a mixed pixel is
modelled as a linear combination of the spectral signatures of the
constituent endmembers, with each end member being assigned a proportion
or fraction that represents its contribution to the overall reflectance
or radiance of the mixed pixel.

\hypertarget{accuracy-assessment}{%
\subsection{Accuracy assessment}\label{accuracy-assessment}}

\begin{itemize}
\item
  Producer accuracy: Recall
\item
  User's accuracy: Precision
\item
  Overall accuracy: not equivalent to F1
\item
  Kappa coefficient: {[}0, 1{]}, measures how good the classification is
  compared to random distribution e.g.~Poisson. Different
  interpretations of this metric, problematic
\end{itemize}

Make a tradeoff between Producer accuracy and User accuracy, by shifting
the decision boundary.

\begin{itemize}
\item
  F1: issue: TN not considered; Recall and precision are not equally
  important yet equally weighted in F1
\item
  Receiver Operating Characteristic Curve: True positive rate and false
  positive rate are all good. We want to maximise the area under the
  curve in a True positive rate vs false positive rate plot.
\end{itemize}

\hypertarget{workflow}{%
\subsection{Workflow}\label{workflow}}

\begin{itemize}
\item
  (Potentially use unsupervised classification to understand your data
\item
  Class definition (Potentially use unsupervised classification)
\item
  Preprocessing
\item
  Training
\item
  Pixel Assignment
\item
  Accuracy assessment
\end{itemize}

Pseudo-invariant features to be trained on to make your model robust to
time-space changes

Pseudo-invariant features are often used as reference targets or
calibration sites in remote sensing to account for changes in sensor or
atmospheric conditions and to reduce the effects of noise and
calibration drift on image data. These features have relatively constant
spectral properties over time and space, and can therefore serve as a
stable reference for monitoring changes in other features or materials
within an image or scene.

A flow chart can be seen in Figure~\ref{fig-flowchart} :

\begin{figure}

{\centering \includegraphics{./images/FlowChart.png}

}

\caption{\label{fig-flowchart}Classification Workflow, courtesy: myself}

\end{figure}

\hypertarget{a-sneak-preview-analogous-to-data-leakage-in-ml}{%
\subsection{A ``Sneak preview'' (Analogous to Data Leakage in
ML)}\label{a-sneak-preview-analogous-to-data-leakage-in-ml}}

Waldo Tobler's first law of geography indicates that if training and
testing are spatially close, the training can cause the problem of a
sneak preview.

\hypertarget{spatial-cross-validation}{%
\subsubsection{Spatial Cross
Validation}\label{spatial-cross-validation}}

Similar to cross-validation but adds clustering to folds.

In spatial cross-validation, the data are split into spatially
contiguous blocks or subsets, rather than randomly shuffled subsets as
in traditional cross-validation. This is done to ensure that the model
is tested on data that are spatially distinct from the data used to
train the model and to account for spatial autocorrelation and other
spatial dependencies in the data.

\hypertarget{approaches-to-deal-with-spatial-autocorrelation}{%
\subsection{Approaches to deal with Spatial
Autocorrelation}\label{approaches-to-deal-with-spatial-autocorrelation}}

Object-based image classification

Moran's I (Spatial Cross Validation)

\hypertarget{application---to-be-completed}{%
\section{Application - to be
completed}\label{application---to-be-completed}}

In remote sensing, it is often challenging to accurately classify mixed
pixels, which contain a combination of different materials or
components. Endmembers refer to pure or nearly pure materials that are
present within a mixed pixel. By modelling the spectral signature of a
mixed pixel as a linear combination of the spectral signatures of the
constituent endmembers, we can determine the contribution of each
endmember to the overall reflectance or radiance of the mixed pixel.

This approach can be very useful in urban recognition, where it is
essential to accurately classify the different land covers present
within a pixel. Furthermore, it can also help us to understand the
composition of the land cover in a given area, which can have important
implications for environmental monitoring and management. This approach
has been applied in various studies to estimate urban land cover, such
as the work by Zhang et al.~(2018) that utilised endmember extraction to
detect urban impervious surfaces.

\hypertarget{reflection-3}{%
\section{Reflection}\label{reflection-3}}

The workflow of Classification of Surface Reflectance and Top of
Atmosphere data intrigues me, as it differs from, yet shares certain
features with traditional computer vision tasks like image
classification and object detection (in regards of treating pixels as
objects/units). Alternatively, Surface Reflectance Classification can be
treated as a downstream task for both aforementioned ML tasks, due to
its uniqueness in dealing with high-precision satellite data and
unseparability (worth debating) from EO processing workflow (calibration
etc.). Also, uncertainty of classification genres derived from
unsupervised labelling can also be an issue.

Optionally, in supplementation to manual labelling, automated labelling
workflow (e.g.~roboflow) can be introduced to curtail repeatitive works
in image labelling(Nair, Paul, and Jacob 2018). However, manual
labelling is not replaceable at current time (Robison 2018) and the
pinning down of ground truth seems to always need human intervention in
addition to machine automation.

The introduction of production accuracy and user accuracy is also
interesting, as these terminologies are designed presupposing a
customer/producer split, dwarfing precision-recall in readability. The
treadeoff to be made between the two is crucial, and this is
problematically handled by introducing F1 score with two competitive
components, and improved by introducing Receiver Operating
Characteristic Curve (ROCC) with two ``good'' indicators, true positive
rate and false positive rate.

\bookmarksetup{startatroot}

\hypertarget{section}{%
\chapter{}\label{section}}

\bookmarksetup{startatroot}

\hypertarget{week8---temperature-and-policy}{%
\chapter{\texorpdfstring{Week8 - T\textbf{emperature and
Policy}}{Week8 - Temperature and Policy}}\label{week8---temperature-and-policy}}

The ``policy'' section occupies two weeks, mainly trying to introduce
how to fit EO data workflow into current policies. To do this, you have
to identify the gaps, e.g., that between the overarching global
policies, metropolitan plans and local plans. Or, the gap within
policies like missing locations in the Singapore one.

\hypertarget{summary-6}{%
\section{Summary}\label{summary-6}}

\hypertarget{urban-heating-islands-uhi-problem-and-plans}{%
\subsection{Urban Heating Islands (UHI) problem and
plans}\label{urban-heating-islands-uhi-problem-and-plans}}

\hypertarget{causes}{%
\subsubsection{Causes}\label{causes}}

Urban areas have comparatively higher atmospheric and surface
temperatures than surrounding rural areas, mainly due to

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  More dark surfaces that retain heat
\item
  Less vegetation that cools the environment
\end{enumerate}

Also, there are other contributors to the heat:

\begin{longtable}[]{@{}ll@{}}
\toprule()
Contributor & Correlation with UHI phenomenon \\
\midrule()
\endhead
Sky View Factor (SVF) & Positive \\
Air speed & Negative \\
Heavy cloud cover & Positive \\
Cyclic solar radiation & Positive \\
Building material type & Varies \\
Anthropogenic energy & Positive \\
\bottomrule()
\end{longtable}

\hypertarget{cost}{%
\subsubsection{Cost}\label{cost}}

The cost of the Urban Heat Island can be divided into social,
environmental, and economic costs.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2442}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.7558}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Type of Cost
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} \\
\midrule()
\endhead
Social Costs & Population-adjusted excess mortality rates, heat-related
deaths \\
Environmental Costs & Increase in fossil fuel usage \\
Economic Costs & Loss of Gross Domestic Product (GDP) \\
\bottomrule()
\end{longtable}

For instance, under a low greenhouse gas scenario, the percent GDP lost
from UHI is estimated to be 0.71\% in 2050.

\hypertarget{plans}{%
\subsubsection{Plans}\label{plans}}

\hypertarget{global}{%
\paragraph{Global}\label{global}}

\hypertarget{local}{%
\paragraph{Local}\label{local}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0846}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3462}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5692}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
City
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Initiatives
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Technology
\end{minipage} \\
\midrule()
\endhead
Singapore & Green buildings & Urban greenery \\
Medellin & Green Corridors & Urban greenery \\
Sydney & Turn Down the Heat Strategy and Action Plan & Reflective
roofs/pavements/sidewalks, Cool roads trial in Western Sydney \\
\bottomrule()
\end{longtable}

\hypertarget{application-2}{%
\section{Application}\label{application-2}}

MacLachlan et al. (2021) The document outlines a sub-city urban planning
modeling approach using open-source tools to measure and monitor
localised urban heat island (UHI) mitigation targets. The methodology
involves comparing temperature dynamics of low- and high-density census
areas using Earth observation data and determining optimal placement of
greening elements in proposed plans using a data-driven model. The
document concludes that this approach can be universally integrated into
urban planning regulation frameworks to mitigate the localized UHI
effect and ensure long-term city sustainability. Also it discusses the
impact of low population density on housing in Perth, Australia, and the
resulting need for strategic land zonation and sustainability targets.
\#\#\#\# Why Data-driven approach

\hypertarget{policy-limitations}{%
\subsection{Policy limitations}\label{policy-limitations}}

\begin{itemize}
\tightlist
\item
  lacking \textbf{specificities} for combating adverse temperature
  effects at the local level (\textbf{sub-city}), therefore not planning
  practicality
\item
  no \textbf{consistency} in planning implementation
  \textbf{methodologies} or steps for \textbf{assessing} progress toward
  UHI reduction targets
\item
  lackage of empirical \textbf{evidence for optimizing} UHI mitigation
  strategies
\end{itemize}

\hypertarget{data-to-drive-the-new-approach}{%
\subsection{Data to drive the new
approach}\label{data-to-drive-the-new-approach}}

\begin{itemize}
\tightlist
\item
  Earth Observation (\textbf{EO}) data can be processed to identify
  (un)sustainable urban development through aerial assessments of
  \textbf{land cover change}
\item
  temperature
\item
  elevation
\end{itemize}

\hypertarget{advantages-for-the-data-driven-approach}{%
\subsection{Advantages for the Data-driven
approach}\label{advantages-for-the-data-driven-approach}}

\begin{itemize}
\tightlist
\item
  EO data can produce \textbf{consistent information} necessary for
  restricting unsustainable development
\item
  \textbf{monitor} UHI effects based on associated land-temperature
  dynamics
\item
  Assessment at finer spatial scales (e.g., block subdivisions)
\end{itemize}

\hypertarget{methodology}{%
\section{Methodology}\label{methodology}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{temperature-modeling}{%
\subsection{Temperature Modeling}\label{temperature-modeling}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  Modeled temperature every 3 hours using SOLWEIG model between 2008 and
  2010
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  Inputs generated from meteorological data, land cover, building DSM,
  ground DTM, and vegetation canopy REM in QGIS
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  SVF computed from vegetation canopy REM, building DSM, and ground DTM
  using UMEP plugin
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  Building wall heights and aspect generated from DSM and DTM using UMEP
  plugin
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{data-driven-tree-placement}{%
\subsection{Data-Driven Tree
Placement}\label{data-driven-tree-placement}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  Site selected for modeling temperature in the City of Fremantle
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  Three scenarios processed: current urban footprint, proposed changes,
  and proposed redevelopment with no trees
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  Highest temperatures identified and used to redesign tree placement
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  15 trees distributed according to original design aspects
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  Updated vegetation canopy REM reflected new tree locations
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  Analysis re-run to compare temperature across redevelopment site
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  Modeled all scenarios accounting for influence of neighboring
  landscape features
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{result}{%
\section{Result}\label{result}}

Table 1: Results of assessments of urban design factors on UHI effect in
Perth.

\begin{longtable}[]{@{}ll@{}}
\toprule()
Factors assessed & Effect on UHI effect \\
\midrule()
\endhead
Vegetation cover & Negative correlation with UHI effect \\
Canopy cover & Negative correlation with UHI effect \\
Building density & Positive correlation with UHI effect \\
Building height & Positive correlation with UHI effect \\
Albedo & Negative correlation with UHI effect \\
Land use & Negative correlation with UHI effect \\
Urban sprawl & Positive correlation with UHI effect \\
\bottomrule()
\end{longtable}

Table 2: Total population and population density per 0.1 km2 between
2011 and 2016 for Subiaco and Currambine SA1s as defined by the ABS.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0769}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1859}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1859}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2756}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2756}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{SA1}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Total Population (2011)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Total Population (2016)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Population Density per 0.1 km2 (2011)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Population Density per 0.1 km2 (2016)}
\end{minipage} \\
\midrule()
\endhead
Subiaco & N/A & N/A & 325 & 712 \\
Currambine & N/A & N/A & 310 & 324 \\
\bottomrule()
\end{longtable}

\hypertarget{reflection-4}{%
\section{Reflection}\label{reflection-4}}

\begin{itemize}
\tightlist
\item
  Case study: Superblocks, Barcelona
\end{itemize}

Basically about pedestrian economy. Though there have been many retail
modes like KFC and other American fast food, the experience from Europe
tells that economy vitality can have a boost with pedestrian-dominated
areas. See Barcelona

\bookmarksetup{startatroot}

\hypertarget{week-9---synthetic-aperture-radar-sar-data}{%
\chapter{\texorpdfstring{Week 9 - \textbf{Synthetic Aperture Radar (SAR)
data}}{Week 9 - Synthetic Aperture Radar (SAR) data}}\label{week-9---synthetic-aperture-radar-sar-data}}

\hypertarget{summary-7}{%
\section{Summary}\label{summary-7}}

This week addresses problems in

\begin{itemize}
\tightlist
\item
  The object of using Synthetic Aperture Radar (SAR)
\end{itemize}

Detecting changes in the Earth's surface over time

\begin{itemize}
\tightlist
\item
  The advantages of SAR for change detection

  \begin{itemize}
  \tightlist
  \item
    see through clouds
  \item
    high temporal resolution
  \end{itemize}
\item
  Techniques for change detection with SAR?

  \begin{itemize}
  \tightlist
  \item
    ratio
  \item
    log ratios between two images
  \item
    t-tests
  \end{itemize}
\item
  fused with other data?
\end{itemize}

Yes, with optical data using techniques such as

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  principal component analysis
\item
  object-based image analysis
\item
  intensity fusion
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Applications?

  \begin{itemize}
  \tightlist
  \item
    monitoring land use changes
  \item
    detecting deforestation
  \item
    identifying urban growth pattern
  \end{itemize}
\end{itemize}

\hypertarget{possible-future-developments}{%
\subsection{Possible future
developments}\label{possible-future-developments}}

\begin{itemize}
\tightlist
\item
  Resolution and accuracy:

  \begin{itemize}
  \tightlist
  \item
    Urban planning: Improved resolution and accuracy can influence local
    zoning regulations and urban growth management by providing detailed
    information on land use changes and the built environment. For
    example, high-resolution SAR data can be used to assess the
    effectiveness of urban containment policies or to identify areas
    where infrastructure investments are needed.
  \end{itemize}
\item
  Data processing capabilities

  \begin{itemize}
  \tightlist
  \item
    Disaster response: The ability to process larger datasets and
    monitor Earth's surface in near real-time can inform global policies
    and agreements related to disaster management, such as the
    \href{https://www.undrr.org/implementing-sendai-framework/what-sendai-framework}{Sendai
    Framework for Disaster Risk Reduction}. Rapid response to natural
    disasters, like earthquakes or hurricanes, can be coordinated more
    effectively with updated SAR data, allowing for quicker deployment
    of resources and better management of affected areas.
  \end{itemize}
\item
  New SAR applications

  \begin{itemize}
  \tightlist
  \item
    Agricultural: Advancements in SAR technology will expand its use in
    change detection and monitoring. This will support policies like the
    \href{https://ec.europa.eu/info/food-farming-fisheries/key-policies/common-agricultural-policy/cap-glance_en}{European
    Union's Common Agricultural Policy (CAP)}, by providing data on crop
    health, irrigation needs, and land use changes.
  \item
    Forestry (deforestation and reforestation tracking)
  \item
    Disaster response (flood and landslide monitoring)
  \item
    Environmental management: SAR data can inform policies related to
    wetland and coastal zone management, such as the
    \href{https://www.ramsar.org/}{Ramsar Convention on Wetlands} and
    the
    \href{https://www.un.org/Depts/los/convention_agreements/convention_overview_convention.htm}{United
    Nations Convention on the Law of the Sea (UNCLOS)}. By monitoring
    changes in these sensitive areas, policymakers can evaluate the
    effectiveness of existing regulations and develop new strategies to
    protect vital ecosystems.
  \end{itemize}
\item
  Machine learning and artificial intelligence:

  \begin{itemize}
  \tightlist
  \item
    Advanced algorithms will be able to identify and classify features
    and changes in SAR data analysis in the Earth's surface, improving
    the overall accuracy of change detection, thus supporting climate
    change adaptation efforts at both local and global levels, including
    the \href{https://unfccc.int/}{United Nations Framework Convention
    on Climate Change (UNFCCC)} and the \href{https://unfccc.int/}{Paris
    Agreement}. Predictive models based on SAR data can help
    policymakers identify areas at risk of flooding, coastal erosion, or
    other climate-related impacts, enabling the development of targeted
    adaptation strategies.
  \end{itemize}
\end{itemize}

In conclusion, advancements in SAR technology, combined with the
integration of machine learning and artificial intelligence, will
significantly enhance the capabilities for change detection and Earth
surface monitoring. These developments will have far-reaching
implications for various sectors, including agriculture, forestry,
disaster management, and environmental protection, ultimately
influencing policymaking and promoting more informed decision-making
processes.

\hypertarget{sar-fundamentals}{%
\subsection{SAR fundamentals}\label{sar-fundamentals}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.0958}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.9042}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Topic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Points
\end{minipage} \\
\midrule()
\endhead
Definition & Synthetic Aperture Radar (SAR) is a type of radar that uses
microwave signals to create high-resolution images of the Earth's
surface. \\
Advantages & Operates in all weather conditions; penetrates through
clouds and vegetation cover. \\
Limitations & Sensitive to surface roughness; limited spatial
resolution. \\
Processing Techniques & Interferometry: combines multiple SAR images to
create 3D maps of the Earth's surface. Polarimetry: analyzes the
polarization properties of reflected signals to extract additional
information about surface features. \\
Applications & Environmental monitoring, disaster response, urban
planning, military surveillance, and more. \\
\bottomrule()
\end{longtable}

\hypertarget{practical-change-detection-with-sar}{%
\subsection{Practical change detection with
SAR}\label{practical-change-detection-with-sar}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3007}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6993}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Topic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Points
\end{minipage} \\
\midrule()
\endhead
Advantages of SAR for Change Detection & Can see through clouds unlike
optical sensors; high temporal resolution. \\
Change Detection Techniques & Ratio or log ratios between two images;
t-tests; standard deviation. \\
Fusion of SAR and Optical Data & Principal component analysis;
object-based image analysis; intensity fusion. \\
Applications of Change Detection with SAR & Monitoring land use changes,
detecting deforestation, identifying urban growth patterns, and more. \\
\bottomrule()
\end{longtable}

\hypertarget{application-3}{%
\section{Application}\label{application-3}}

\bookmarksetup{startatroot}

\hypertarget{summary-8}{%
\chapter{Summary}\label{summary-8}}

In summary, this book has no content whatsoever.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \OperatorTok{+} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
2
\end{verbatim}

\bookmarksetup{startatroot}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-google_machine_2023}{}}%
Google. 2023a. {``Machine {Learning} in {Earth} {Engine} {\textbar}
{Google} {Earth} {Engine}.''} \emph{Google Developers}.
\url{https://developers.google.com/earth-engine/guides/machine-learning}.

\leavevmode\vadjust pre{\hypertarget{ref-google_reducer_2023}{}}%
---------. 2023b. {``Reducer {Overview} {\textbar} {Google} {Earth}
{Engine}.''} \emph{Google Developers}.
\url{https://developers.google.com/earth-engine/guides/reducers_intro}.

\leavevmode\vadjust pre{\hypertarget{ref-gorelick_google_2017}{}}%
Gorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David
Thau, and Rebecca Moore. 2017. {``Google {Earth} {Engine}:
{Planetary}-Scale Geospatial Analysis for Everyone.''} \emph{Remote
Sensing of Environment}, Big {Remotely} {Sensed} {Data}: Tools,
applications and experiences, 202 (December): 18--27.
\url{https://doi.org/10.1016/j.rse.2017.06.031}.

\leavevmode\vadjust pre{\hypertarget{ref-Jensen1986IntroductoryDI}{}}%
Jensen, J. Robert. 1986. {``Introductory Digital Image Processing: A
Remote Sensing Perspective.''} In.

\leavevmode\vadjust pre{\hypertarget{ref-maclachlan_sustainable_2021}{}}%
MacLachlan, Andrew, Eloise Biggs, Gareth Roberts, and Bryan Boruff.
2021. {``Sustainable {City} {Planning}: {A} {Data}-{Driven} {Approach}
for {Mitigating} {Urban} {Heat}.''} \emph{Frontiers in Built
Environment} 6.
\url{https://www.frontiersin.org/articles/10.3389/fbuil.2020.519599}.

\leavevmode\vadjust pre{\hypertarget{ref-nair2018automated}{}}%
Nair, Rahul, P. J. Paul, and K. Poulose Jacob. 2018. {``Automated Image
Annotation: A Survey.''} \emph{Journal of Imaging} 4 (3): 37.

\leavevmode\vadjust pre{\hypertarget{ref-robison2018future}{}}%
Robison, Keela. 2018. {``The Future of Image Annotation: Human in the
Loop.''}
\url{https://lionbridge.ai/articles/the-future-of-image-annotation-human-in-the-loop/}.

\leavevmode\vadjust pre{\hypertarget{ref-saad_el_imanni_multispectral_2023}{}}%
Saad El Imanni, Hajar, Abderrazak El Harti, El Mostafa Bachaoui, Hicham
Mouncif, Fatine Eddassouqui, Mohamed Achraf Hasnai, and Moulay Ismail
Zinelabidine. 2023. {``Multispectral {UAV} Data for Detection of Weeds
in a Citrus Farm Using Machine Learning and {Google} {Earth} {Engine}:
{Case} Study of {Morocco}.''} \emph{Remote Sensing Applications: Society
and Environment} 30 (April): 100941.
\url{https://doi.org/10.1016/j.rsase.2023.100941}.

\leavevmode\vadjust pre{\hypertarget{ref-schulte_to_buhne_better_2018}{}}%
Schulte to B端hne, Henrike, and Nathalie Pettorelli. 2018. {``Better
Together: {Integrating} and Fusing Multispectral and Radar Satellite
Imagery to Inform Biodiversity Monitoring, Ecological Research and
Conservation Science.''} \emph{Methods in Ecology and Evolution} 9 (4):
849--65. \url{https://doi.org/10.1111/2041-210X.12942}.

\end{CSLReferences}



\end{document}
